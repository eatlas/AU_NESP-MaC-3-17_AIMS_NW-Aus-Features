"""
# Validation Location Generator for Reef Features Dataset

## Purpose
This script generates validation locations for reef features in Northwest Australia.
The purpose is to create standardized validation points that can be used to assess
the quality and consistency of reef feature mapping across different versions of the
dataset. By creating these reference points, we can automatically validate updates
to the mapped features without requiring manual re-evaluation.

## Multi-Validator Support
The script creates separate output files for each validator person (identified by their
initials, e.g., 'EL', 'RB'). Each validator receives an identical set of initial
validation datasets, which they can then modify independently. This approach:

1. Allows comparison between different validators' assessments
2. Prevents mix-ups between different validators' work
3. Organizes files in validator-specific directories

For each validator, the files are saved in this structure:
working/20/{validator_initials}/NW-Aus-Features-v0-4_Feature-centroid-{batch}_{validator_initials}.shp

## Methodology
The script implements a stratified random sampling approach:

1. **Spatial Stratification**: 
   - The study area is divided into 12 regions using the validation regions shapefile
   - Features are assigned to regions based on their centroids
   - For each region, a fixed number of features (10 by default) are randomly selected
   - This ensures validation points are distributed throughout the study area

2. **Three Validation Datasets**:
   - **Feature-centroid**: Points at the center of each feature with validation attributes
     (FeatExists, TypeConf, RB_Type_L3, FeatConf, Attachment)
     This allows the person developing the validation dataset to record the attributes of 
     the feature being validated. These attributes can then be compared with the mapped features
     and other people that have also validated the same feature.
   - **Polygon-extent**: Either a simplified and randomly fuzzed polygon representing feature boundaries,
     or a bounding box of the fuzzed polygon (depending on a global setting).
     The bounding box option provides the validator with only the approximate size and location of the 
     feature, minimizing bias in interpreting the boundary.
   - **Boundary-error**: Random points along feature boundaries (not on land)
     - Small features (<1km perimeter): 1 boundary point
     - Larger features: 3 boundary points
     The intention is that the person developing the validation dataset will adjust these points
     to the most accurate estimate of the true edge of the boundary being mapped.

3. **Inclusion of Fake (Open Water) Locations to Reduce Sampling Bias**:
   - To reduce sampling bias (where validators may expect a feature at every validation location), a
     proportion of the validation locations are intentionally placed in open water where no mapped feature exists.
   - These "fake" locations are generated by:
     - Creating a mask by buffering all mapped reef features and removing any area overlapping land, 
       ensuring fake locations are near real features but not on land.
     - Randomly sampling points within this mask to serve as fake feature centroids.
     - Assigning each fake feature a random size (with a bias toward smaller features) and aspect ratio, 
       and generating a bounding box polygon and boundary-error points as for real features.
   - The proportion of fake features is controlled by a global constant (default 30% of the total 
     validation set).
   - Fake and real features are intermixed in the output datasets, making it difficult for validators 
     to know which locations are real or fake, thus reducing confirmation bias.
   - A separate shapefile of fake centroids is also saved for later analysis, but the main validation 
     dataset does not indicate which points are fake.

4. **Processing Steps**:
   - Features are simplified using a tolerance of 50m
   - Random fuzzing (Â±50m) is applied to vertices for more realistic validation
   - For polygon-extent, a global constant determines whether to output the fuzzed polygon or its bounding box
   - For boundary points, we check against a coastline dataset to ensure they're not on land
   - If a point is on land, we retry up to a maximum number of attempts

5. **Batch Processing**:
   - Features are processed in batches, with each batch creating its own set of files
   - Each batch uses different features to ensure broader coverage
   - A fixed random seed ensures reproducibility between runs

## Outputs
The script produces three shapefiles per batch:
   - NW-Aus-Features-v0-4_Feature-centroid-XX.shp
   - NW-Aus-Features-v0-4_Polygon-extent-XX.shp
   - NW-Aus-Features-v0-4_Boundary-error-XX.shp

Where XX is the batch number (01, 02, etc.)

Additionally, a shapefile of the fake location mask and a shapefile of the fake centroids are saved for review and analysis.

These validation datasets can be used to automatically assess:
   - Whether features exist in the expected locations
   - Whether feature type classification is consistent
   - How accurately boundaries are mapped relative to validated boundary points
   - The rate of false positives (features marked as present at fake locations)
"""

import geopandas as gpd
import numpy as np
import random
import os
import configparser
from shapely.geometry import Point, LineString, Polygon, MultiPolygon
import pandas as pd
from tqdm import tqdm

# Load config file to get base path
config = configparser.ConfigParser()
config.read('config.ini')
BASE_PATH = config.get('general', 'in_3p_path')

# Constants
# This collection was made to act as an example to explain how the validation
# works. These results are reviewed by all validations and thus are not
# intended to be used for validation of the dataset.
RANDOM_SEED = 315
NUM_BATCHES = 1
FEATURES_PER_REGION = 1
VALIDATORS = ['Examples']  

# This collection is used for the production validation dataset.
# RANDOM_SEED = 42  
# NUM_BATCHES = 10
# FEATURES_PER_REGION = 10

# Initials of the people who will validate the dataset
# This will create a copy of the validation data per person
# VALIDATORS = ['EL', 'RB']  


SIMPLIFY_TOLERANCE = 50  # meters
FUZZ_DISTANCE = 100  # meters for vertex fuzzing
SMALL_BOUNDARY_POINTS_PER_FEATURE = 1 
LARGE_BOUNDARY_POINTS_PER_FEATURE = 1 
SMALL_SIZE_THRESHOLD_M = 2000  # Threshold for small features in meters
MAX_ATTEMPTS_PER_POINT = 10  # Maximum attempts to find a point not on land

# Global constant to control whether to use bounding box for polygon-extent
POLYGON_EXTENT_USE_BOUNDING_BOX = True  # Set to True to use bounding box, False for fuzzed polygon

# File paths
VALIDATION_REGIONS_FILE = "data/v0-4/in/NW-Aus-Features-validation-regions.shp"
FEATURES_FILE = "data/v0-4/in/Reef-Boundaries_v0-4_edit.shp"
COASTLINE_FILE = f"{BASE_PATH}/AU_AIMS_Coastline_50k_2024/Split/AU_NESP-MaC-3-17_AIMS_Aus-Coastline-50k_2024_V1-1_split.shp"
OUTPUT_DIR = "working/20"

FAKE_FEATURE_PROPORTION = 0.3  # Proportion of fake features (open water) in the validation set
FAKE_ATTEMPT_LIMIT = 1000      # Max attempts for rejection sampling fake locations

def load_data():
    """Load the validation regions, reef features, and coastline datasets"""
    print("Loading validation regions...")
    regions = gpd.read_file(VALIDATION_REGIONS_FILE)
    print(f"Loaded {len(regions)} validation regions with CRS: {regions.crs}")
    
    print("Loading reef features...")
    features = gpd.read_file(FEATURES_FILE)
    print(f"Loaded {len(features)} reef features with CRS: {features.crs}")
    
    print("Loading coastline...")
    coastline = gpd.read_file(COASTLINE_FILE)
    print(f"Loaded {len(coastline)} coastline features with CRS: {coastline.crs}")
    
    # Print bounding boxes for debugging
    print(f"Features bounding box: {features.total_bounds}")
    print(f"Regions bounding box: {regions.total_bounds}")
    
    return regions, features, coastline

def assign_features_to_regions(features, regions):
    """Assign features to regions based on their centroids"""
    print("Assigning features to regions...")
    # Calculate centroids for spatial join
    features_centroids = features.copy()
    features_centroids['geometry'] = features_centroids.geometry.centroid
    
    print(f"Feature centroids CRS: {features_centroids.crs}")
    print(f"Regions CRS: {regions.crs}")
    
    # Check if CRS are compatible or need transformation
    if features_centroids.crs != regions.crs:
        print(f"WARNING: CRS mismatch! Converting features from {features_centroids.crs} to {regions.crs}")
        features_centroids = features_centroids.to_crs(regions.crs)
    
    # Spatial join to assign region
    features_with_region = gpd.sjoin(features_centroids, regions, how="left", predicate="within")
    
    # Count how many features were assigned to regions
    unassigned = features_with_region[features_with_region['RegionID'].isna()]
    assigned = features_with_region[~features_with_region['RegionID'].isna()]
    print(f"Features assigned to regions: {len(assigned)} of {len(features_with_region)}")
    print(f"Features NOT assigned to any region: {len(unassigned)}")
    
    if len(assigned) == 0:
        print("ERROR: No features were assigned to any region!")
        print("Checking if features intersect with regions at all...")
        # Try a different spatial predicate
        intersects_join = gpd.sjoin(features_centroids, regions, how="left", predicate="intersects")
        intersects_count = len(intersects_join[~intersects_join['RegionID'].isna()])
        print(f"Features that intersect with regions: {intersects_count}")
        
        # Sample a few features and check their coordinates
        print("Sample feature centroid coordinates:")
        sample_size = min(5, len(features_centroids))
        for i, (idx, row) in enumerate(features_centroids.iloc[:sample_size].iterrows()):
            print(f"  Feature {i}: {row.geometry.x}, {row.geometry.y}")
    
    # Group features by region
    features_by_region = {}
    for region_id in regions['RegionID'].unique():
        region_features = features_with_region[features_with_region['RegionID'] == region_id]
        if not region_features.empty:
            # Get the original geometries for these features
            region_indices = region_features.index
            region_features = features.loc[region_indices].copy()
            features_by_region[region_id] = region_features
            print(f"Region {region_id}: {len(region_features)} features")
        else:
            # Create a proper empty GeoDataFrame with a geometry column
            features_by_region[region_id] = gpd.GeoDataFrame({'geometry': []}, crs=features.crs)
            print(f"Region {region_id}: No features")
    
    return features_by_region

def convert_to_crs_units(distance_m, crs):
    """Convert distance from meters to units of the CRS"""
    if crs.is_geographic:  # If the CRS is in degrees (geographic)
        # Rough approximation - 1 degree is about 111 km at the equator
        return distance_m / 111000
    else:  # Projected CRS - units are likely meters already
        return distance_m

def simplify_and_fuzz_polygon(polygon, crs):
    """Simplify a polygon and apply random fuzzing to its vertices, then return either the fuzzed polygon or its bounding box."""
    # Convert distances to appropriate units
    tolerance_units = convert_to_crs_units(SIMPLIFY_TOLERANCE, crs)
    fuzz_distance_units = convert_to_crs_units(FUZZ_DISTANCE, crs)
    
    # Simplify the polygon
    simplified = polygon.simplify(tolerance_units)
    
    # Apply fuzzing to vertices
    def fuzz_coords(coords):
        return [(x + random.uniform(-fuzz_distance_units, fuzz_distance_units),
                 y + random.uniform(-fuzz_distance_units, fuzz_distance_units)) for x, y in coords]
    
    if simplified.geom_type == 'Polygon':
        exterior_coords = list(simplified.exterior.coords)
        fuzzed_coords = fuzz_coords(exterior_coords)
        fuzzed_polygon = Polygon(fuzzed_coords)
    elif simplified.geom_type == 'MultiPolygon':
        fuzzed_parts = []
        for part in simplified.geoms:
            exterior_coords = list(part.exterior.coords)
            fuzzed_coords = fuzz_coords(exterior_coords)
            fuzzed_parts.append(Polygon(fuzzed_coords))
        fuzzed_polygon = MultiPolygon(fuzzed_parts)
    else:
        fuzzed_polygon = simplified  # fallback
    
    if POLYGON_EXTENT_USE_BOUNDING_BOX:
        # Return the bounding box as a Polygon
        minx, miny, maxx, maxy = fuzzed_polygon.bounds
        return Polygon([
            (minx, miny),
            (minx, maxy),
            (maxx, maxy),
            (maxx, miny),
            (minx, miny)
        ])
    else:
        return fuzzed_polygon

def generate_boundary_points(polygon, crs, coastline):
    """
    Generate random points along the boundary of a polygon,
    excluding points that overlap with land areas.
    Number of points depends on feature size:
    - Small features (perimeter < 1km): 1 point
    - Larger features: 3 points
    """
    # Get the boundary of the polygon
    boundary = polygon.boundary
    
    # Calculate perimeter length in meters (approx)
    perimeter = 0
    if boundary.geom_type == 'LineString':
        perimeter = boundary.length
    elif boundary.geom_type == 'MultiLineString':
        perimeter = sum(line.length for line in boundary.geoms)
    else:
        print(f"Warning: Unexpected geometry type: {boundary.geom_type}")
        return []
    
    # If using geographic coordinates, convert to approximate meters
    if crs.is_geographic:
        # Rough conversion: 1 degree ~ 111km at equator
        perimeter = perimeter * 111000
    
    # Determine number of points based on size
    num_points = SMALL_BOUNDARY_POINTS_PER_FEATURE \
        if perimeter < SMALL_SIZE_THRESHOLD_M else LARGE_BOUNDARY_POINTS_PER_FEATURE
    
    # Handle different geometry types
    if boundary.geom_type == 'LineString':
        lines = [boundary]
    elif boundary.geom_type == 'MultiLineString':
        lines = list(boundary.geoms)
    else:
        print(f"Warning: Unexpected geometry type: {boundary.geom_type}")
        return []
    
    # Calculate the total length of all line segments
    total_length = sum(line.length for line in lines)
    
    # Generate points and check against coastline
    valid_points = []
    attempts = 0
    max_attempts = num_points * MAX_ATTEMPTS_PER_POINT
    
    while len(valid_points) < num_points and attempts < max_attempts:
        # Choose a random position along the total length
        pos = random.uniform(0, total_length)
        
        # Find which line segment this position falls on
        current_length = 0
        point = None
        
        for line in lines:
            if pos <= current_length + line.length:
                # This is the line that contains our point
                position_on_line = pos - current_length
                point = line.interpolate(position_on_line)
                break
            current_length += line.length
        
        if point is not None:
            # Check if the point overlaps with land
            if not coastline.contains(point).any():
                valid_points.append(point)
        
        attempts += 1
    
    if len(valid_points) < num_points:
        print(f"Warning: Could only generate {len(valid_points)} valid boundary points " 
              f"out of {num_points} requested after {attempts} attempts.")
    
    return valid_points

def ensure_centroid_inside(polygon):
    """Ensure the centroid is inside the polygon, or find a point that is"""
    centroid = polygon.centroid
    if polygon.contains(centroid):
        return centroid
    
    # If centroid is not inside, use a point on surface
    return polygon.representative_point()

def create_fake_location_mask(features, coastline):
    """
    Create a mask polygon for generating fake features:
    - Dissolve all features
    - Simplify (0.001 deg)
    - Buffer by 0.05 deg (~5km)
    - Remove any area that overlaps land (difference with coastline)
    - Simplify again (0.001 deg)
    All in EPSG:4326.
    """
    print("Creating fake location mask...")
    features_4326 = features.to_crs("EPSG:4326")
    coastline_4326 = coastline.to_crs("EPSG:4326")
    # Dissolve all features into one geometry
    print("Dissolving reef features into a single geometry.")
    dissolved = features_4326.dissolve()
    dissolved_geom = dissolved.geometry.unary_union if hasattr(dissolved, "geometry") else dissolved.unary_union
    # Simplify
    print("Simplifying dissolved geometry.")
    simplified = dissolved_geom.simplify(0.001)
    # Buffer by 0.05 degrees
    buffered = simplified.buffer(0.05)
    print("Removing land from buffered reef area to create mask.")
    # Coastline may be multipart, so union all
    coastline_union = coastline_4326.unary_union
    # Remove land from buffered area (difference)
    mask = buffered.difference(coastline_union)
    # Simplify again
    mask = mask.simplify(0.001)
    print("Fake location mask created.")
    return mask

def sample_point_in_mask(mask, attempt_limit=FAKE_ATTEMPT_LIMIT):
    """
    Rejection sample a random point inside the mask polygon (EPSG:4326).
    Returns a shapely Point or None if failed.
    """
    # Ensure mask is a shapely geometry, not a GeoSeries or DataFrame
    if hasattr(mask, "geometry"):
        mask_geom = mask.geometry.unary_union
    else:
        mask_geom = mask
    minx, miny, maxx, maxy = map(float, mask_geom.bounds)
    for _ in range(attempt_limit):
        x = random.uniform(minx, maxx)
        y = random.uniform(miny, maxy)
        pt = Point(x, y)
        if mask_geom.contains(pt):
            return pt
    print("Warning: Failed to sample a fake feature location after max attempts.")
    return None

def random_fake_feature_size():
    """
    Returns (width_m, aspect_ratio) for a fake feature.
    Width is 50-2000m, with more weight to small values (squared uniform).
    Aspect ratio is uniform between 0.5 and 2.
    """
    u = random.uniform(0, 1)
    width = 50 + (2000 - 50) * (u ** 2)
    aspect = random.uniform(0.5, 2.0)
    return width, aspect

def create_fake_feature_polygon(center, width_m, aspect, crs):
    """
    Create an axis-aligned rectangle polygon centered at 'center' (Point, EPSG:4326),
    with given width (m) and aspect ratio. Returns a shapely Polygon in EPSG:4326.
    """
    # Convert width and height from meters to degrees (approx)
    width_deg = width_m / 111000
    height_deg = (width_m / aspect) / 111000
    cx, cy = center.x, center.y
    minx = cx - width_deg / 2
    maxx = cx + width_deg / 2
    miny = cy - height_deg / 2
    maxy = cy + height_deg / 2
    return Polygon([
        (minx, miny),
        (minx, maxy),
        (maxx, maxy),
        (maxx, miny),
        (minx, miny)
    ])

def fuzz_fake_centroid(center, width_m, height_m):
    """
    Fuzz the centroid by up to 30% of width/height (in meters, converted to degrees).
    """
    fuzz_x = (random.uniform(-0.3, 0.3)) * width_m
    fuzz_y = (random.uniform(-0.3, 0.3)) * height_m
    dx = fuzz_x / 111000
    dy = fuzz_y / 111000
    return Point(center.x + dx, center.y + dy)

def generate_fake_features(num_fake, mask, crs, coastline):
    """
    Generate fake features (open water) as dicts for centroids, extents, and boundary errors.
    Returns: (centroids, extents, boundary_errors, fake_centroids_only)
    """
    centroids = []
    extents = []
    boundary_errors = []
    fake_centroids_only = []

    for _ in range(num_fake):
        pt = sample_point_in_mask(mask)
        if pt is None:
            continue
        width_m, aspect = random_fake_feature_size()
        height_m = width_m / aspect
        fuzzed_centroid = fuzz_fake_centroid(pt, width_m, height_m)
        poly = create_fake_feature_polygon(fuzzed_centroid, width_m, aspect, crs)
        centroid_data = {
            'ValidID': None,  # Will assign after shuffle
            'FeatExists': None,
            'FeatConf': None,
            'RB_Type_L3': None,
            'TypeConf': None,
            'Attachment': None,
            'geometry': fuzzed_centroid,
            'fake': True
        }
        centroids.append(centroid_data)
        extent_data = {
            'ValidID': None,  # Will assign after shuffle
            'geometry': poly
        }
        extents.append(extent_data)
        boundary_pts = generate_boundary_points(poly, crs, coastline)
        for bpt in boundary_pts:
            boundary_errors.append({
                'ValidID': None, 
                'EdgeAcc_m': None,  # Placeholder for edge accuracy
                'geometry': bpt
            })  # Will assign after shuffle
    return centroids, extents, boundary_errors

def create_batch_datasets(features_by_region, batch_num, used_features_indices, coastline, fake_location_mask, crs):
    """Create the three validation datasets for a batch, including fake features."""
    print(f"Creating validation datasets for batch {batch_num}...")

    # Collect real features
    real_centroids = []
    real_extents = []
    real_boundary_errors = []
    for region_id, features in features_by_region.items():
        if len(features) == 0:
            print(f"Warning: No features found in region {region_id}")
            continue
        if region_id not in used_features_indices:
            used_features_indices[region_id] = set()
        available_indices = [idx for idx in features.index if idx not in used_features_indices[region_id]]
        num_to_select = min(FEATURES_PER_REGION, len(available_indices))
        if num_to_select == 0:
            print(f"Warning: No features left in region {region_id} for batch {batch_num}")
            continue
        selected_indices = np.random.choice(available_indices, size=num_to_select, replace=False)
        selected_features = features.loc[selected_indices]
        used_features_indices[region_id].update(selected_indices)
        for _, feature in selected_features.iterrows():
            inside_point = ensure_centroid_inside(feature.geometry)

            #print the inside point position for debugging
            # print(f"Inside point for feature in region {region_id}: {inside_point.x}, {inside_point.y}")

            centroid_data = {
                'ValidID': None,  # Will assign after shuffling
                'FeatExists': None,
                'FeatConf': None,
                'RB_Type_L3': None,
                'TypeConf': None,
                'Attachment': None,
                'geometry': inside_point,
                'fake': False
            }
            real_centroids.append(centroid_data)
            simplified_fuzzed = simplify_and_fuzz_polygon(feature.geometry, features.crs)
            extent_data = {
                'ValidID': None,  # Will assign after shuffling
                'geometry': simplified_fuzzed
            }
            real_extents.append(extent_data)
            boundary_points = generate_boundary_points(simplified_fuzzed, features.crs, coastline)
            for point in boundary_points:
                boundary_data = {
                    'ValidID': None,  # Will assign after shuffling
                    'EdgeAcc_m': None,  # Placeholder for edge accuracy
                    'geometry': point
                }
                real_boundary_errors.append(boundary_data)
            # valid_id += 1  # Remove this, will assign after shuffle

    num_real = len(real_centroids)
    num_fake = int(np.round(FAKE_FEATURE_PROPORTION * num_real / (1 - FAKE_FEATURE_PROPORTION)))
    print(f"Generating {num_fake} fake features for {num_real} real features (proportion {FAKE_FEATURE_PROPORTION:.2f})")
    # Generate fake features with placeholder ValidID=None
    fake_centroids, fake_extents, fake_boundary_errors = generate_fake_features(
        num_fake, fake_location_mask, crs, coastline
    )

    # Combine and shuffle while keeping alignment
    all_centroids = real_centroids + fake_centroids
    all_extents = real_extents + fake_extents
    all_boundary_errors = real_boundary_errors + fake_boundary_errors
    combined = list(zip(all_centroids, all_extents, all_boundary_errors))
    random.shuffle(combined)
    all_centroids, all_extents, all_boundary_errors = zip(*combined) if combined else ([], [], [])

    # Assign sequential ValidIDs after shuffle
    for idx, (centroid, extent, boundary) in enumerate(zip(all_centroids, all_extents, all_boundary_errors), 1):
        centroid['ValidID'] = idx
        extent['ValidID'] = idx
        boundary['ValidID'] = idx  # Note: if multiple boundary points per feature, this only sets the first; fix below

    # Fix ValidID for all boundary errors (since there may be multiple per feature)
    # Build a mapping from geometry to ValidID for centroids
    centroid_geom_to_validid = {id(centroid['geometry']): centroid['ValidID'] for centroid in all_centroids}
    # For each boundary error, assign ValidID based on the corresponding centroid's geometry object
    for boundary in all_boundary_errors:
        # Find the matching centroid by geometry object id
        # This assumes the boundary error comes from the same feature as the centroid in the zipped list
        # If not, fallback to sequential assignment
        if boundary['ValidID'] is None:
            # Try to find matching centroid by geometry object id
            # (This works because the zipped lists are aligned)
            idx = all_boundary_errors.index(boundary)
            if idx < len(all_centroids):
                boundary['ValidID'] = all_centroids[idx]['ValidID']

    # Update fake_centroids_only ValidIDs to match shuffled centroids
    fake_centroids_gdf = []
    for centroid in all_centroids:
        if centroid.get('fake'):
            fake_centroids_gdf.append({'ValidID': centroid['ValidID'], 'geometry': centroid['geometry']})

    # Convert to GeoDataFrames
    feature_centroids_gdf = gpd.GeoDataFrame(list(all_centroids), crs=crs)
    feature_centroids_gdf = feature_centroids_gdf[['ValidID', 'FeatExists', 'FeatConf', 'RB_Type_L3', 'TypeConf', 'Attachment', 'geometry', 'fake']]
    polygon_extents_gdf = gpd.GeoDataFrame(list(all_extents), crs=crs)
    boundary_errors_gdf = gpd.GeoDataFrame(list(all_boundary_errors), crs=crs)
    fake_centroids_gdf = gpd.GeoDataFrame(fake_centroids_gdf, crs=crs)[['ValidID', 'geometry']]

    return feature_centroids_gdf, polygon_extents_gdf, boundary_errors_gdf, fake_centroids_gdf

def save_validation_datasets(centroids, extents, boundary_points, batch_num, fake_centroids=None):
    """
    Save the validation datasets to shapefiles for each validator.
    Also saves a separate shapefile of fake centroids if provided.
    """
    batch_str = str(batch_num).zfill(2)
    for validator in VALIDATORS:
        validator_dir = os.path.join(OUTPUT_DIR, validator)
        os.makedirs(validator_dir, exist_ok=True)
        centroids_file = os.path.join(validator_dir, f"NW-Aus-Features-v0-4_Feature-centroid-{batch_str}_{validator}.shp")
        extents_file = os.path.join(validator_dir, f"NW-Aus-Features-v0-4_Polygon-extent-{batch_str}_{validator}.shp")
        boundary_file = os.path.join(validator_dir, f"NW-Aus-Features-v0-4_Boundary-error-{batch_str}_{validator}.shp")
        # Remove 'fake' column before saving main centroids
        centroids_to_save = centroids.drop(columns=['fake'])
        print(f"Saving Feature-centroid for validator {validator} to {centroids_file}")
        centroids_to_save.to_file(centroids_file)
        print(f"Saving Polygon-extent for validator {validator} to {extents_file}")
        extents.to_file(extents_file)
        print(f"Saving Boundary-error for validator {validator} to {boundary_file}")
        boundary_points.to_file(boundary_file)
        # Save fake centroids shapefile
        if fake_centroids is not None:
            fake_file = os.path.join(validator_dir, f"NW-Aus-Features-v0-4_Fake-centroid-{batch_str}_{validator}.shp")
            print(f"Saving Fake-centroid for validator {validator} to {fake_file}")
            fake_centroids.to_file(fake_file)

def main():
    """Main function to generate validation locations"""
    print("Starting validation location generation...")
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    print(f"Using random seed: {RANDOM_SEED}")
    # Load data
    regions, features, coastline = load_data()
    # Reproject all to EPSG:4326 for mask/fake feature logic
    features = features.to_crs("EPSG:4326")
    coastline = coastline.to_crs("EPSG:4326")
    regions = regions.to_crs("EPSG:4326")
    # Assign features to regions
    features_by_region = assign_features_to_regions(features, regions)
    # Create fake location mask
    fake_location_mask = create_fake_location_mask(features, coastline)
    # Save fake location mask as shapefile for review
    mask_gdf = gpd.GeoDataFrame(geometry=[fake_location_mask], crs="EPSG:4326")
    mask_file = os.path.join(OUTPUT_DIR, "NW-Aus-Features-v0-4_Fake-location-mask.shp")
    print(f"Saving fake location mask to {mask_file}")
    mask_gdf.to_file(mask_file)
    used_features_indices = {}
    for batch in range(1, NUM_BATCHES + 1):
        print(f"\nProcessing batch {batch} of {NUM_BATCHES}")
        centroids, extents, boundary_points, fake_centroids = create_batch_datasets(
            features_by_region, batch, used_features_indices, coastline, fake_location_mask, features.crs
        )
        save_validation_datasets(centroids, extents, boundary_points, batch, fake_centroids)
    print("\nValidation location generation complete!")

if __name__ == "__main__":
    main()